{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "p = '/nrs/ahrens/ahrenslab/fleishmang/functional_alex_early_march/20210303'\n",
    "fix_directory = p + '/green_anatomy_after_20210303_123132'\n",
    "mov_directory = p + '/spont4um_20210303_121349'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get fixed image mean\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CircuitSeeker import motion_correct\n",
    "import nrrd\n",
    "\n",
    "# path information for folder of time frames\n",
    "frames = {'folder':fix_directory,\n",
    "          'prefix':'TM',\n",
    "          'suffix':'.h5',\n",
    "          'dataset_path':'/default'}\n",
    "\n",
    "# compute mean from all frames\n",
    "fix = motion_correct.distributed_image_mean(\n",
    "    frames,\n",
    "    cluster_kwargs={'project':'ahrens', 'cores':4, 'processes':1},\n",
    ")\n",
    "\n",
    "# store output - switch to xyz axis order\n",
    "nrrd.write('./fix.nrrd', fix.transpose(2,1,0), compression_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute fixed image mask (optional)\n",
    "---\n",
    "Later, when we apply the transforms that motion correct all the time frames, we have the option to apply a mask to each frame. This will reduce the dataset size on disk by about 60% - depending on your data that could be several terabytes.\n",
    "\n",
    "You should run this cell a few times varying the value of `lambda2`. Larger values will make the mask bigger, smaller values will make the mask smaller. Each time you should look at the mask on top of the fixed image (written out above) and proceed with a mask that covers the brain area entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CircuitSeeker.level_set import brain_detection\n",
    "from scipy.ndimage import zoom, binary_dilation, binary_closing\n",
    "import numpy as np\n",
    "\n",
    "# the voxel spacings for the fixed and time series data\n",
    "# these are essential to get right - make sure you know them\n",
    "fix_spacing = np.array([1.0, 0.406, 0.406])\n",
    "mov_spacing = np.array([4.0, 0.406, 0.406])\n",
    "\n",
    "# we segment on downsampled data to make the segmentation sufficiently fast\n",
    "fix_small = zoom(fix, [0.5, 0.25, 0.25], order=1).transpose(2,1,0)\n",
    "fix_small_spacing = fix_spacing[::-1] * [4, 4, 2]\n",
    "\n",
    "# segment\n",
    "mask = brain_detection(\n",
    "    fix_small,\n",
    "    fix_small_spacing,\n",
    "    smooth_sigmas=[12, 6, 4],\n",
    "    lambda2=4,\n",
    "    mask_smoothing=2,\n",
    ")\n",
    "\n",
    "# dilate the boundaries a little, go back to original sampling, and smooth boundaries\n",
    "# you can also play with the dilation/closing element size here to adjust mask boundaries\n",
    "mask = binary_dilation(mask, np.ones((10,10,10))).astype(np.uint8)\n",
    "mask = zoom(mask, np.array(fix.shape[::-1]) / fix_small.shape, order=0)\n",
    "mask = binary_closing(mask, np.ones((5,5,5))).astype(np.uint8)\n",
    "\n",
    "# save the result\n",
    "nrrd.write('./mask.nrrd', mask, compression_level=2)\n",
    "mask = mask.transpose(2,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion correct\n",
    "---\n",
    "Two important parameters here are `time_stride` and `sigma`. `time_stride` is the sub-sampling in time for which you want to correct, i.e. `time_stride=10` means only rigid align every 10th frame and interpolate to find the transforms for the frames in between. `sigma` is the standard deviation of a Gaussian applied to the transform parameters _over time_. This stabilizes the motion correction. When you increase `time_stride` you should _decrease_ sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster dashboard link:  http://10.36.110.12:8787/status\n",
      "Scaling cluster to 100 workers with 1 cores per worker\n",
      "*** This cluster costs 7.0 dollars per hour starting now ***\n",
      "Waiting 30 seconds for cluster to scale\n",
      "Wait time complete\n"
     ]
    }
   ],
   "source": [
    "# information regarding the time series data\n",
    "frames = {'folder':mov_directory,\n",
    "          'prefix':'TM',\n",
    "          'suffix':'.h5',\n",
    "          'dataset_path':'/default'}\n",
    "\n",
    "# the voxel spacings for the fixed and time series data\n",
    "# these are essential to get right - make sure you know them\n",
    "fix_spacing = np.array([1.0, 0.406, 0.406])\n",
    "mov_spacing = np.array([4.0, 0.406, 0.406])\n",
    "\n",
    "# motion correct\n",
    "# this will launch a dask cluster and print some useful information\n",
    "# you should watch the dashboard to get a sense of all the computations happening\n",
    "# `transforms` will contain a 4x4 rigid transform matrix for every time frame\n",
    "transforms = motion_correct.motion_correct(\n",
    "    fix, frames,\n",
    "    fix_spacing, mov_spacing,\n",
    "    time_stride=10,\n",
    "    sigma=0.75,\n",
    "    cluster_kwargs={\n",
    "        'project':'ahrens',\n",
    "        'cores':1, 'processes':1,\n",
    "        'max_workers':100,\n",
    "    },\n",
    ")\n",
    "\n",
    "# write the transforms out as individual files for storage\n",
    "transforms_folder = './transforms'\n",
    "motion_correct.save_transforms(\n",
    "    transforms, transforms_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply transforms\n",
    "---\n",
    "Note we are using the `mask` to reduce the overall size of the written dataset here.\n",
    "\n",
    "An important parameter here is `config`. This allows you to change the [dask configuration](https://docs.dask.org/en/latest/configuration-reference.html). Very large resample jobs may require configuration changes in order for the cluster to handle the large amount of computation without shutting down. You should always test these functions using a small dataset first.\n",
    "\n",
    "For example - you can provide a keyword argument here `subset` which is a python `slice` object. This will specify the subset of frames that you actually want to transform: `subset = slice(100, 200, 10)` will only transform time points 100, 110, 120, ... 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster dashboard link:  http://10.36.110.12:8787/status\n",
      "Scaling cluster to 200 workers with 1 cores per worker\n",
      "*** This cluster costs 14.0 dollars per hour starting now ***\n",
      "Waiting 30 seconds for cluster to scale\n",
      "Wait time complete\n"
     ]
    }
   ],
   "source": [
    "# the motion corrected dataset will be written here as a zarr file\n",
    "write_path = './motion_corrected.zarr'\n",
    "\n",
    "# This requires some heavy computation\n",
    "# set `cores` and `max_workers` carefully - you need to be aware of the\n",
    "# resource cost of your job\n",
    "aligned_frames = motion_correct.resample_frames(\n",
    "    frames,\n",
    "    mov_spacing,\n",
    "    transforms,\n",
    "    write_path,\n",
    "    mask=mask,\n",
    "    cluster_kwargs={\n",
    "        'project':'ahrens',\n",
    "        'cores':1, 'processes':1,\n",
    "        'max_workers':200,\n",
    "        'config':{},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a time slice (optional)\n",
    "---\n",
    "Take a look at one slice of your data over time. We're taking every other time point so that the file isn't huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to slice in time/space\n",
    "time_stride, plane = 2, 30\n",
    "\n",
    "# get the data from the zarr file\n",
    "slice_over_time = aligned_frames[::time_stride, plane, :, :]\n",
    "\n",
    "# write out in a format you can read with Fiji/Icy etc.\n",
    "nrrd.write('./slice_over_time.nrrd', slice_over_time.transpose(2,1,0), compression_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
